{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "99dfcce9-61e3-4221-8abc-a986ed655b3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "import logging\n",
    "import logging\n",
    "import os\n",
    "import torch\n",
    "import transformers\n",
    "from dataclasses import dataclass, field\n",
    "from datasets import load_dataset\n",
    "from functools import partial\n",
    "from torch.utils.data import Dataset\n",
    "from tqdm import tqdm\n",
    "from transformers import DataCollatorForSeq2Seq\n",
    "from transformers import Trainer\n",
    "from typing import Dict, Optional, Sequence\n",
    "from typing import List"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "088d0222-1447-44a7-99ee-37593dc9ec6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "logger = logging.getLogger(__name__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "51367301-5909-4faa-8451-42d51348267e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_all_datapath(dir_name: str) -> List[str]:\n",
    "    all_file_list = []\n",
    "    # all_file_size = []\n",
    "\n",
    "    for (root, dir, file_name) in os.walk(dir_name):\n",
    "        for temp_file in file_name:\n",
    "            standard_path = f\"{root}/{temp_file}\"\n",
    "\n",
    "            all_file_list.append(standard_path)\n",
    "\n",
    "    return all_file_list\n",
    "\n",
    "\n",
    "def load_dataset_from_path(data_path: Optional[str] = None,\n",
    "                           cache_dir: Optional[str] = \"cache_data\") -> Dataset:\n",
    "    all_file_list = get_all_datapath(data_path)\n",
    "    data_files = {'train': all_file_list}\n",
    "    extension = all_file_list[0].split(\".\")[-1]\n",
    "\n",
    "    logger.info(\"load files %d number\", len(all_file_list))\n",
    "\n",
    "    raw_datasets = load_dataset(\n",
    "        extension,\n",
    "        data_files=data_files,\n",
    "        cache_dir=cache_dir,\n",
    "    )['train']\n",
    "    return raw_datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b6af7b93-301c-4911-a281-f52e1c0ab505",
   "metadata": {},
   "outputs": [],
   "source": [
    "IGNORE_INDEX = -100\n",
    "PROMPT_DICT = {\n",
    "    \"prompt_input\": (\n",
    "        \"Below is an instruction that describes a task, paired with an input that provides further context. \"\n",
    "        \"Write a response that appropriately completes the request.\\n\\n\"\n",
    "        \"### Instruction:\\n{instruction}\\n\\n### Input:\\n{input}\\n\\n### Response:\"\n",
    "    ),\n",
    "    \"prompt_no_input\": (\n",
    "        \"Below is an instruction that describes a task. \"\n",
    "        \"Write a response that appropriately completes the request.\\n\\n\"\n",
    "        \"### Instruction:\\n{instruction}\\n\\n### Response:\"\n",
    "    ),\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f35ef452-d18b-4e9b-8f40-1e0c58d02710",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class ModelArguments:\n",
    "    model_name_or_path: Optional[str] = field(default=\"facebook/opt-125m\")\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class DataArguments:\n",
    "    data_path: str = field(default=None, metadata={\n",
    "        \"help\": \"Path to the training data.\"})\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class TrainingArguments(transformers.TrainingArguments):\n",
    "    cache_dir: Optional[str] = field(default=None)\n",
    "    optim: str = field(default=\"adamw_torch\")\n",
    "    model_max_length: int = field(\n",
    "        default=512,\n",
    "        metadata={\n",
    "            \"help\": \"Maximum sequence length. Sequences will be right padded (and possibly truncated).\"},\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "acf97d56-f2e7-4f67-8896-5d1d9a87e63d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _tokenize_fn(strings: Sequence[str], tokenizer: transformers.PreTrainedTokenizer) -> Dict:\n",
    "    \"\"\"Tokenize a list of strings.\"\"\"\n",
    "    tokenized_list = [\n",
    "        tokenizer(\n",
    "            text,\n",
    "            return_tensors=\"pt\",\n",
    "            padding=\"longest\",\n",
    "            max_length=tokenizer.model_max_length,\n",
    "            truncation=True,\n",
    "        )\n",
    "        for text in strings\n",
    "    ]\n",
    "    input_ids = labels = [tokenized.input_ids[0]\n",
    "                          for tokenized in tokenized_list]\n",
    "    ne_pad_token_id = IGNORE_INDEX if tokenizer.pad_token_id is None else tokenizer.pad_token_id\n",
    "    input_ids_lens = labels_lens = [\n",
    "        tokenized.input_ids.ne(ne_pad_token_id).sum().item() for tokenized in tokenized_list\n",
    "    ]\n",
    "    return dict(\n",
    "        input_ids=input_ids,\n",
    "        labels=labels,\n",
    "        input_ids_lens=input_ids_lens,\n",
    "        labels_lens=labels_lens,\n",
    "    )\n",
    "\n",
    "\n",
    "def preprocess(\n",
    "        sources: Sequence[str],\n",
    "        targets: Sequence[str],\n",
    "        tokenizer: transformers.PreTrainedTokenizer,\n",
    ") -> Dict:\n",
    "    \"\"\"Preprocess the data by tokenizing.\"\"\"\n",
    "    examples = [s + t for s, t in zip(sources, targets)]\n",
    "    examples_tokenized, sources_tokenized = [_tokenize_fn(\n",
    "        strings, tokenizer) for strings in (examples, sources)]\n",
    "    input_ids = examples_tokenized[\"input_ids\"]\n",
    "    labels = copy.deepcopy(input_ids)\n",
    "    for label, source_len in zip(labels, sources_tokenized[\"input_ids_lens\"]):\n",
    "        label[:source_len] = IGNORE_INDEX\n",
    "    return dict(input_ids=input_ids, labels=labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1996344a-a28c-450a-b842-0170f20d947c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_train_dataset(tokenizer: transformers.PreTrainedTokenizer, data_path: str) -> Dataset:\n",
    "    logging.warning(\"Loading data...\")\n",
    "\n",
    "    dataset = load_dataset_from_path(\n",
    "        data_path=data_path,\n",
    "    )\n",
    "    logging.warning(\"Formatting inputs...\")\n",
    "    prompt_input, prompt_no_input = PROMPT_DICT[\"prompt_input\"], PROMPT_DICT[\"prompt_no_input\"]\n",
    "\n",
    "    def generate_sources_targets(examples: Dict, tokenizer: transformers.PreTrainedTokenizer):\n",
    "        ins_data = examples['instruction']\n",
    "        input_data = examples['input']\n",
    "        output = examples['output']\n",
    "\n",
    "        len_ = len(ins_data)\n",
    "\n",
    "        # sources = []\n",
    "        # targets = []\n",
    "\n",
    "        # for i in range(len_):\n",
    "        #     s_t = prompt_input.format_map({'instruction':ins_data[i],\n",
    "        #                                    'input':input_data[i]}) if input_data[i] != \"\" else prompt_input.format_map({'instruction':ins_data[i]})\n",
    "        #     sources.append(s_t)\n",
    "\n",
    "        sources = [prompt_input.format_map({'instruction': ins_data[i], 'input': input_data[i]}) if input_data[\n",
    "                                                                                                        i] != \"\" else prompt_no_input.format_map(\n",
    "            {'instruction': ins_data[i]})\n",
    "                   for i in range(len_)]\n",
    "        targets = [\n",
    "            f\"{example}{tokenizer.eos_token}\" for example in output]\n",
    "\n",
    "        # sources = [prompt_input.format_map(example) if example.get(\"input\", \"\") != \"\" else prompt_no_input.format_map(example)\n",
    "        #            for example in examples]\n",
    "        # targets = [\n",
    "        #     f\"{example['output']}{tokenizer.eos_token}\" for example in examples]\n",
    "\n",
    "        input_output = preprocess(\n",
    "            sources=sources, targets=targets, tokenizer=tokenizer)\n",
    "        examples['input_ids'] = input_output['input_ids']\n",
    "        examples['labels'] = input_output['labels']\n",
    "        return examples\n",
    "\n",
    "    generate_sources_targets_p = partial(\n",
    "        generate_sources_targets, tokenizer=tokenizer)\n",
    "\n",
    "    dataset = dataset.map(\n",
    "        function=generate_sources_targets_p,\n",
    "        batched=True,\n",
    "        desc=\"Running tokenizer on train dataset\",\n",
    "        num_proc=2\n",
    "    ).shuffle()\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2e46edb9-d166-4e66-9e11-387b685dcee8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train():\n",
    "    parser = transformers.HfArgumentParser(\n",
    "        (ModelArguments, DataArguments, TrainingArguments))\n",
    "    model_args, data_args, training_args = parser.parse_args_into_dataclasses()\n",
    "\n",
    "    model = transformers.AutoModelForCausalLM.from_pretrained(\n",
    "        model_args.model_name_or_path,\n",
    "        cache_dir=training_args.cache_dir,\n",
    "        device_map='auto',\n",
    "        torch_dtype=torch.bfloat16,\n",
    "        trust_remote_code=True if model_args.model_name_or_path.find(\"falcon\") != -1 else False\n",
    "\n",
    "    )\n",
    "    model.is_parallelizable = True\n",
    "    model.model_parallel = True\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "    tokenizer = transformers.AutoTokenizer.from_pretrained(\n",
    "        model_args.model_name_or_path,\n",
    "        cache_dir=training_args.cache_dir,\n",
    "        model_max_length=training_args.model_max_length,\n",
    "        padding_side=\"right\",\n",
    "        use_fast=False,\n",
    "    )\n",
    "    if model_args.model_name_or_path.find(\"falcon\") != -1:\n",
    "        tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "    train_dataset = make_train_dataset(\n",
    "        tokenizer=tokenizer, data_path=data_args.data_path)\n",
    "\n",
    "    data_collator = DataCollatorForSeq2Seq(tokenizer=tokenizer, model=model,\n",
    "                                           label_pad_token_id=IGNORE_INDEX\n",
    "                                           )\n",
    "\n",
    "    trainer = Trainer(model=model,\n",
    "                      tokenizer=tokenizer,\n",
    "                      args=training_args,\n",
    "                      train_dataset=train_dataset,\n",
    "                      eval_dataset=None,\n",
    "                      data_collator=data_collator)\n",
    "    trainer.train()\n",
    "    trainer.save_state()\n",
    "    trainer.save_model(output_dir=training_args.output_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e2e87f91-29bb-4a56-a96c-d08588ca5af6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "usage: ipykernel_launcher.py [-h] [--model_name_or_path MODEL_NAME_OR_PATH]\n",
      "                             [--data_path DATA_PATH] --output_dir OUTPUT_DIR\n",
      "                             [--overwrite_output_dir [OVERWRITE_OUTPUT_DIR]]\n",
      "                             [--do_train [DO_TRAIN]] [--do_eval [DO_EVAL]]\n",
      "                             [--do_predict [DO_PREDICT]]\n",
      "                             [--evaluation_strategy {no,steps,epoch}]\n",
      "                             [--prediction_loss_only [PREDICTION_LOSS_ONLY]]\n",
      "                             [--per_device_train_batch_size PER_DEVICE_TRAIN_BATCH_SIZE]\n",
      "                             [--per_device_eval_batch_size PER_DEVICE_EVAL_BATCH_SIZE]\n",
      "                             [--per_gpu_train_batch_size PER_GPU_TRAIN_BATCH_SIZE]\n",
      "                             [--per_gpu_eval_batch_size PER_GPU_EVAL_BATCH_SIZE]\n",
      "                             [--gradient_accumulation_steps GRADIENT_ACCUMULATION_STEPS]\n",
      "                             [--eval_accumulation_steps EVAL_ACCUMULATION_STEPS]\n",
      "                             [--eval_delay EVAL_DELAY]\n",
      "                             [--learning_rate LEARNING_RATE]\n",
      "                             [--weight_decay WEIGHT_DECAY]\n",
      "                             [--adam_beta1 ADAM_BETA1]\n",
      "                             [--adam_beta2 ADAM_BETA2]\n",
      "                             [--adam_epsilon ADAM_EPSILON]\n",
      "                             [--max_grad_norm MAX_GRAD_NORM]\n",
      "                             [--num_train_epochs NUM_TRAIN_EPOCHS]\n",
      "                             [--max_steps MAX_STEPS]\n",
      "                             [--lr_scheduler_type {linear,cosine,cosine_with_restarts,polynomial,constant,constant_with_warmup,inverse_sqrt,reduce_lr_on_plateau}]\n",
      "                             [--lr_scheduler_kwargs LR_SCHEDULER_KWARGS]\n",
      "                             [--warmup_ratio WARMUP_RATIO]\n",
      "                             [--warmup_steps WARMUP_STEPS]\n",
      "                             [--log_level {detail,debug,info,warning,error,critical,passive}]\n",
      "                             [--log_level_replica {detail,debug,info,warning,error,critical,passive}]\n",
      "                             [--log_on_each_node [LOG_ON_EACH_NODE]]\n",
      "                             [--no_log_on_each_node]\n",
      "                             [--logging_dir LOGGING_DIR]\n",
      "                             [--logging_strategy {no,steps,epoch}]\n",
      "                             [--logging_first_step [LOGGING_FIRST_STEP]]\n",
      "                             [--logging_steps LOGGING_STEPS]\n",
      "                             [--logging_nan_inf_filter [LOGGING_NAN_INF_FILTER]]\n",
      "                             [--no_logging_nan_inf_filter]\n",
      "                             [--save_strategy {no,steps,epoch}]\n",
      "                             [--save_steps SAVE_STEPS]\n",
      "                             [--save_total_limit SAVE_TOTAL_LIMIT]\n",
      "                             [--save_safetensors [SAVE_SAFETENSORS]]\n",
      "                             [--no_save_safetensors]\n",
      "                             [--save_on_each_node [SAVE_ON_EACH_NODE]]\n",
      "                             [--save_only_model [SAVE_ONLY_MODEL]]\n",
      "                             [--no_cuda [NO_CUDA]] [--use_cpu [USE_CPU]]\n",
      "                             [--use_mps_device [USE_MPS_DEVICE]] [--seed SEED]\n",
      "                             [--data_seed DATA_SEED]\n",
      "                             [--jit_mode_eval [JIT_MODE_EVAL]]\n",
      "                             [--use_ipex [USE_IPEX]] [--bf16 [BF16]]\n",
      "                             [--fp16 [FP16]] [--fp16_opt_level FP16_OPT_LEVEL]\n",
      "                             [--half_precision_backend {auto,apex,cpu_amp}]\n",
      "                             [--bf16_full_eval [BF16_FULL_EVAL]]\n",
      "                             [--fp16_full_eval [FP16_FULL_EVAL]] [--tf32 TF32]\n",
      "                             [--local_rank LOCAL_RANK]\n",
      "                             [--ddp_backend {nccl,gloo,mpi,ccl,hccl}]\n",
      "                             [--tpu_num_cores TPU_NUM_CORES]\n",
      "                             [--tpu_metrics_debug [TPU_METRICS_DEBUG]]\n",
      "                             [--debug DEBUG [DEBUG ...]]\n",
      "                             [--dataloader_drop_last [DATALOADER_DROP_LAST]]\n",
      "                             [--eval_steps EVAL_STEPS]\n",
      "                             [--dataloader_num_workers DATALOADER_NUM_WORKERS]\n",
      "                             [--dataloader_prefetch_factor DATALOADER_PREFETCH_FACTOR]\n",
      "                             [--past_index PAST_INDEX] [--run_name RUN_NAME]\n",
      "                             [--disable_tqdm DISABLE_TQDM]\n",
      "                             [--remove_unused_columns [REMOVE_UNUSED_COLUMNS]]\n",
      "                             [--no_remove_unused_columns]\n",
      "                             [--label_names LABEL_NAMES [LABEL_NAMES ...]]\n",
      "                             [--load_best_model_at_end [LOAD_BEST_MODEL_AT_END]]\n",
      "                             [--metric_for_best_model METRIC_FOR_BEST_MODEL]\n",
      "                             [--greater_is_better GREATER_IS_BETTER]\n",
      "                             [--ignore_data_skip [IGNORE_DATA_SKIP]]\n",
      "                             [--fsdp FSDP]\n",
      "                             [--fsdp_min_num_params FSDP_MIN_NUM_PARAMS]\n",
      "                             [--fsdp_config FSDP_CONFIG]\n",
      "                             [--fsdp_transformer_layer_cls_to_wrap FSDP_TRANSFORMER_LAYER_CLS_TO_WRAP]\n",
      "                             [--accelerator_config ACCELERATOR_CONFIG]\n",
      "                             [--deepspeed DEEPSPEED]\n",
      "                             [--label_smoothing_factor LABEL_SMOOTHING_FACTOR]\n",
      "                             [--optim OPTIM] [--optim_args OPTIM_ARGS]\n",
      "                             [--adafactor [ADAFACTOR]]\n",
      "                             [--group_by_length [GROUP_BY_LENGTH]]\n",
      "                             [--length_column_name LENGTH_COLUMN_NAME]\n",
      "                             [--report_to REPORT_TO [REPORT_TO ...]]\n",
      "                             [--ddp_find_unused_parameters DDP_FIND_UNUSED_PARAMETERS]\n",
      "                             [--ddp_bucket_cap_mb DDP_BUCKET_CAP_MB]\n",
      "                             [--ddp_broadcast_buffers DDP_BROADCAST_BUFFERS]\n",
      "                             [--dataloader_pin_memory [DATALOADER_PIN_MEMORY]]\n",
      "                             [--no_dataloader_pin_memory]\n",
      "                             [--dataloader_persistent_workers [DATALOADER_PERSISTENT_WORKERS]]\n",
      "                             [--skip_memory_metrics [SKIP_MEMORY_METRICS]]\n",
      "                             [--no_skip_memory_metrics]\n",
      "                             [--use_legacy_prediction_loop [USE_LEGACY_PREDICTION_LOOP]]\n",
      "                             [--push_to_hub [PUSH_TO_HUB]]\n",
      "                             [--resume_from_checkpoint RESUME_FROM_CHECKPOINT]\n",
      "                             [--hub_model_id HUB_MODEL_ID]\n",
      "                             [--hub_strategy {end,every_save,checkpoint,all_checkpoints}]\n",
      "                             [--hub_token HUB_TOKEN]\n",
      "                             [--hub_private_repo [HUB_PRIVATE_REPO]]\n",
      "                             [--hub_always_push [HUB_ALWAYS_PUSH]]\n",
      "                             [--gradient_checkpointing [GRADIENT_CHECKPOINTING]]\n",
      "                             [--gradient_checkpointing_kwargs GRADIENT_CHECKPOINTING_KWARGS]\n",
      "                             [--include_inputs_for_metrics [INCLUDE_INPUTS_FOR_METRICS]]\n",
      "                             [--fp16_backend {auto,apex,cpu_amp}]\n",
      "                             [--push_to_hub_model_id PUSH_TO_HUB_MODEL_ID]\n",
      "                             [--push_to_hub_organization PUSH_TO_HUB_ORGANIZATION]\n",
      "                             [--push_to_hub_token PUSH_TO_HUB_TOKEN]\n",
      "                             [--mp_parameters MP_PARAMETERS]\n",
      "                             [--auto_find_batch_size [AUTO_FIND_BATCH_SIZE]]\n",
      "                             [--full_determinism [FULL_DETERMINISM]]\n",
      "                             [--torchdynamo TORCHDYNAMO]\n",
      "                             [--ray_scope RAY_SCOPE]\n",
      "                             [--ddp_timeout DDP_TIMEOUT]\n",
      "                             [--torch_compile [TORCH_COMPILE]]\n",
      "                             [--torch_compile_backend TORCH_COMPILE_BACKEND]\n",
      "                             [--torch_compile_mode TORCH_COMPILE_MODE]\n",
      "                             [--dispatch_batches DISPATCH_BATCHES]\n",
      "                             [--split_batches SPLIT_BATCHES]\n",
      "                             [--include_tokens_per_second [INCLUDE_TOKENS_PER_SECOND]]\n",
      "                             [--include_num_input_tokens_seen [INCLUDE_NUM_INPUT_TOKENS_SEEN]]\n",
      "                             [--neftune_noise_alpha NEFTUNE_NOISE_ALPHA]\n",
      "                             [--optim_target_modules OPTIM_TARGET_MODULES]\n",
      "                             [--cache_dir CACHE_DIR]\n",
      "                             [--model_max_length MODEL_MAX_LENGTH]\n",
      "ipykernel_launcher.py: error: the following arguments are required: --output_dir\n"
     ]
    },
    {
     "ename": "SystemExit",
     "evalue": "2",
     "output_type": "error",
     "traceback": [
      "An exception has occurred, use %tb to see the full traceback.\n",
      "\u001b[0;31mSystemExit\u001b[0m\u001b[0;31m:\u001b[0m 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/miniconda3/envs/nlp_train/lib/python3.11/site-packages/IPython/core/interactiveshell.py:3585: UserWarning: To exit: use 'exit', 'quit', or Ctrl-D.\n",
      "  warn(\"To exit: use 'exit', 'quit', or Ctrl-D.\", stacklevel=1)\n"
     ]
    }
   ],
   "source": [
    "logging.basicConfig(\n",
    "        format=\"%(asctime)s %(levelname)s [%(name)s] %(message)s\", level=logging.INFO, datefmt=\"%Y-%m-%d %H:%M:%S\"\n",
    "    )\n",
    "train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "181ef647-4142-4105-958e-4030e4c78441",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
