问题,回答,所属分类
别再问我什么是跳跃表了,"大家好，我是帅地。
跳跃表的代码可以写不出来，但原理以及应用必须懂，例如 Redis 的有序列表就是用跳跃表实现的，秋招那会还被面试官问过好几次，于是有了这篇文章的到来。
假如我们要用某种数据结构来维护一组有序的 int 型数据的集合，并且希望这个数据结构在插入、删除、查找等操作上能够尽可能着快速，那么，你会用什么样的数据结构呢？
一种很简单的方法应该就是采用数组了，在查找方面，用数组存储的话，采用二分法可以在 O(logn) 的时间里找到指定的元素，不过数组在插入、删除这些操作中比较不友好，找到目标位置所需时间为 O(logn) ，进行插入和删除这个动作所需的时间复杂度为 O(n) ，因为都需要移动移动元素，所以最终所需要的时间复杂度为 O(n) 。
例如对于下面这个数组：

插入元素 3

另外一种简单的方法应该就是用链表了，链表在插入、删除的支持上就相对友好，当我们找到目标位置之后，插入、删除元素所需的时间复杂度为 O(1) ，注意，我说的是找到目标位置之后，插入、删除的时间复杂度才为 O(1)。
但链表在查找上就不友好了，不能像数组那样采用二分查找的方式，只能一个一个结点遍历，所以加上查找所需的时间，插入、删除所需的总的时间复杂度为O(n)。
假如我们能够提高链表的查找效率，使链表的查找的时间复杂度尽可能接近 O(logn) ，那链表将会是很棒的选择。
那链表的查找速度可以提高吗？
对于下面这个链表

假如我们要查找元素 9，按道理我们需要从头结点开始遍历，一共遍历8个结点才能找到元素9。
那么能否采取某些策略，让我们遍历 5 次以内就找到元素9呢？请大家花一分钟时间想一下如何实现。
由于元素的有序的，我们是可以通过增加一些路径来加快查找速度的。例如

我们可以先通过比较上面一层的元素来确定目标元素所在区间，通过这种方法，我们只需要遍历 5 次就可以找到元素 9 了（红色的线为查找路径，）。

还能继续加快查找速度吗？
答是可以的，再增加一层就行了，这样只需要4次就能找到了。
这就如同我们搭地铁的时候，去某个站点时，有快线和慢线几种路线，通过快线 + 慢线的搭配，我们可以更快着到达某个站点。

当然，还能在增加一层，

基于这种方法，对于具有 n 个元素的链表，我们可以采取 (logn + 1) 层指针路径的形式，就可以实现在 O(logn) 的时间复杂度内，查找到某个目标元素了。
这种数据结构，我们也称之为跳跃表，跳跃表也可以算是链表的一种变形，只是它具有二分查找的功能。
上面例子中，9 个结点，一共 4 层，可以说是理想的跳跃表了，不过随着我们对跳跃表进行插入/删除结点的操作，那么跳跃表结点数就会改变，意味着跳跃表的层数也会动态改变。
这里我们面临一个问题，就是新插入的结点应该跨越多少层？
这个问题已经有大牛替我们解决好了，采取的策略是通过抛硬币来决定新插入结点跨越的层数：每次我们要插入一个结点的时候，就来抛硬币，如果抛出来的是正面，则继续抛，直到出现负面为止，统计这个过程中出现正面的次数，这个次数作为结点跨越的层数。
通过这种方法，可以尽可能着接近理想的层数。大家可以想一下为啥会这样呢？
（1）、插入
例如，我们要插入结点 3，4，通过抛硬币知道3，4跨越的层数分别为 0，2 (层数从0开始算)，则插入的过程如下：
插入 3，跨越0层。

插入 4，跨越2层。

（2）、删除
解决了插入之后，我们来看看删除，删除就比较简单了，例如我们要删除4，那我们直接把4及其所跨越的层数删除就行了。

跳跃表的插入与删除至此都讲完了，总结下跳跃表的有关性质：
(1). 跳跃表的每一层都是一条有序的链表.
(2). 跳跃表的查找次数近似于层数，时间复杂度为O(logn)，插入、删除也为 O(logn)。
(3). 最底层的链表包含所有元素。
(4). 跳跃表是一种随机化的数据结构(通过抛硬币来决定层数)。
(5). 跳跃表的空间复杂度为 O(n)。
有人可能会说，也可以采用二叉查找树啊，因为查找查找树的插入、删除、查找也是近似 O(logn) 的时间复杂度。
不过，二叉查找树是有可能出现一种极端的情况的，就是如果插入的数据刚好一直有序，那么所有节点会偏向某一边。例如

这种接结构会导致二叉查找树的查找效率变为 O(n),这会使二叉查找树大打折扣。
####七、跳跃表 vs 红黑树
红黑可以说是二叉查找树的一种变形，红黑在查找，插入，删除也是近似O(logn)的时间复杂度，但学过红黑树的都知道，红黑树比跳跃表复杂多了，反正我是被红黑树虐过。在选择一种数据结构时，有时候也是需要考虑学习成本的。
而且红黑树插入，删除结点时，是通过调整结构来保持红黑树的平衡，比起跳跃表直接通过一个随机数来决定跨越几层，在时间复杂度的花销上是要高于跳跃表的。
当然，红黑树并不是一定比跳跃表差，在有些场合红黑树会是更好的选择，所以选择一种数据结构，关键还得看场合。
总上所述，维护一组有序的集合，并且希望在查找、插入、删除等操作上尽可能快，那么跳跃表会是不错的选择。redis 中的数据数据便是采用了跳跃表，当然，ridis也结合了哈希表等数据结构，采用的是一种复合数据结构。
代码如下
如果可以，建议大家写一波代码，目前帅地只更新了 Java 的代码，后续会补上其他语言代码
基础数据结构",
别再问我什么事AVL 树了,"这篇文章通过对话的形式，由浅入深带你读懂 AVL 树，看完让你保证理解 AVL 树的各种操作，如果觉得不错，别吝啬你的赞哦。




1、若它的左子树不为空，则左子树上所有的节点值都小于它的根节点值。
2、若它的右子树不为空，则右子树上所有的节点值均大于它的根节点值。
3、它的左右子树也分别可以充当为二叉查找树。
例如：



例如，我现在想要查找数值为14的节点。由于二叉查找树的特性，我们可以很快着找到它，其过程如下：
>

  1、和根节点9比较
 
  2、由于 14 > 9，所以14只可能存在于9的右子树中，因此查看右孩子13
 
  3、由于 14 > 13，所以继续查看13的右孩子15
 
  4、由于 14 < 15，所以14只可能存在于15的左孩子中，因此查找15的左孩子14
 
  5、这时候发现14正是自己查找的值，于是查找结束。
  这种查找二叉树的查找正是二分查找的思想，可以很快着找到目的节点，查找所需的最大次数等同于二叉查找树的高度。
  在插入的时候也是一样，通过一层一层的比较，最后找到适合自己的位置。
 
 
 
  初始的二叉查找树只有三个节点：
 
  然后我们按照顺序陆续插入节点 4，3，2，1，0。插入之后的结构如下：
 
 
 
 
 
 
  这是一种比查找二叉树还特别的树哦，这种树就可以帮助我们解决二叉查找树刚才的那种所有节点都倾向一边的缺点的。具有如下特性：
每个节点的左子树和右子树的高度差至多等于1。
例如：图一就是一颗AVL树了，而图二则不是(节点右边标的是这个节点的高度)。
 
 
  对于图二，因为节点9的左孩子高度为2，而右孩子高度为0。他们之间的差值超过1了。
  这种树就可以保证不会出现大量节点偏向于一边的情况了。
 
  听起来这种树还不错，可以对于图1，如果我们要插入一个节点3，按照查找二叉树的特性，我们只能把3作为节点4的左子树插进去，可是插进去之后，又会破坏了AVL树的特性，那我们那该怎么弄？
 
  我们在进行节点插入的时候，可能会出现节点都倾向于左边的情况，例如：
 
  我们把这种倾向于左边的情况称之为 左-左型。这个时候，我们就可以对节点9进行右旋操作，使它恢复平衡。
 
  即：顺时针旋转两个节点，使得父节点被自己的左孩子取代，而自己成为自己的右孩子
  再举个例子：
 
  节点4和9高度相差大于1。由于是左孩子的高度较高，此时是左-左型，进行右旋。
 
  这里要注意，节点4的右孩子成为了节点6的左孩子了
  我找了个动图，尽量这个动图和上面例子的节点不一样。
 
  左旋和右旋一样，就是用来解决当大部分节点都偏向右边的时候，通过左旋来还原。例如：
 
  我们把这种倾向于右边的情况称之为 右-右型。
  我也找了一张动图。
 
 
 
  初始状态如下：
 
  然后我们主键插入如下数值：1,4,5,6,7,10,9,8
  插入 1
 
  左-左型，需要右旋调整。
 
  插入4
 
  继续插入 5
 
  右-右型，需要左旋转调整。
 
  继续插入6
 
  右-右型，需要进行左旋
 
  继续插入7
 
  右-右型，需要进行左旋
 
  继续插入10
 
  继续插入9
 
  出现了这种情况怎么办呢?对于这种  右-左型 的情况，单单一次左旋或右旋是不行的，下面我们先说说如何处理这种情况。
 
  这种我们就把它称之为 右-左 型吧。处理的方法是先对节点10进行右旋把它变成右-右型。
 
  然后在进行左旋。
 
  所以对于这种 右-左型的，我们需要进行一次右旋再左旋。
  同理，也存在 左-右型的，例如：
 
  对于左-右型的情况和刚才的 右-左型相反，我们需要对它进行一次左旋，再右旋。
 
  回到刚才那道题
 
  对它进行右旋再左旋。
 
  到此，我们的插入就结束了。
 
 
  在插入的过程中，会出现一下四种情况破坏AVL树的特性，我们可以采取如下相应的旋转。
  1、左-左型：做右旋。
  2、右-右型：做左旋转。
  3、左-右型：先做左旋，后做右旋。
  4、右-左型：先做右旋，再做左旋。
  不知道大家发现规律没，这个规则还是挺好记。
 
 
 
 
基础数据结构",
别再问我什么是红黑树了,"这篇文章算是我写博客写公众号以来画图最多的一篇文章了，没有之一，我希望尽可能多地用图片来形象地描述红黑树的各种操作的前后变换原理，帮助大家来理解红黑树的工作原理，下面，多图预警开始了。
在讲红黑树之前，我们首先来了解下下面几个概念：二叉树，排序二叉树以及平衡二叉树。
二叉树指的是每个节点最多只能有两个子树的有序树。通常左边的子树称为左子树 ，右边的子树称为右子树。这里说的有序树强调的是二叉树的左子树和右子树的次序不能随意颠倒。
二叉树简单的示意图如下：

代码定义：
二、排序二叉树
所谓排序二叉树，顾名思义，排序二叉树是有顺序的，它是一种特殊结构的二叉树，我们可以对树中所有节点进行排序和检索。
性质
排序二叉树简单示意图：

排序二叉树的左子树上所有节点的值小于根节点的值，右子树上所有节点的值大于根节点的值，当我们插入一组元素正好是有序的时候，这时会让排序二叉树退化成链表。
正常情况下，排序二叉树是如下图这样的：

但是，当插入的一组元素正好是有序的时候，排序二叉树就变成了下边这样了，就变成了普通的链表结构，如下图所示:

正常情况下的排序二叉树检索效率类似于二分查找，二分查找的时间复杂度为 O(log n)，但是如果排序二叉树退化成链表结构，那么检索效率就变成了线性的 O(n) 的，这样相对于 O(log n) 来说，检索效率肯定是要差不少的。
思考，二分查找和正常的排序二叉树的时间复杂度都是 O(log n)，那么为什么是O(log n) ？
关于 O(log n) 的分析下面这篇文章讲解的非常好，感兴趣的可以看下这篇文章 二分查找的时间复杂度，文章是拿二分查找来举例的，二分查找和平衡二叉树的时间复杂度是一样的，理解了二分查找的时间复杂度，再来理解平衡二叉树就不难了，这里就不赘述了。
继续回到我们的主题上，为了解决排序二叉树在特殊情况下会退化成链表的问题（链表的检索效率是 O(n) 相对正常二叉树来说要差不少），所以有人发明了平衡二叉树和红黑树类似的平衡树。
平衡二叉数又被称为 AVL 树，AVL 树的名字来源于它的发明作者 G.M. Adelson-Velsky 和 E.M. Landis，取自两人名字的首字母。
官方定义：它或者是一颗空树，或者具有以下性质的排序二叉树：它的左子树和右子树的深度之差(平衡因子)的绝对值不超过1，且它的左子树和右子树都是一颗平衡二叉树。
两个条件：
讲了这么多概念，接下来主角红黑树终于要上场了。
为什么有红黑树？
其实红黑树和上面的平衡二叉树类似，本质上都是为了解决排序二叉树在极端情况下退化成链表导致检索效率大大降低的问题，红黑树最早是由 Rudolf Bayer 于 1972 年发明的。
红黑树首先肯定是一个排序二叉树，它在每个节点上增加了一个存储位来表示节点的颜色，可以是 RED 或 BLACK 。
Java 中实现红黑树大概结构图如下所示：

针对上面的 5 种性质，我们简单理解下，对于性质 1 和性质 2 ，相当于是对红黑树每个节点的约束，根节点是黑色，其他的节点要么是红色，要么是黑色。
对于性质 3 中指定红黑树的每个叶子节点都是空节点，而且叶子节点都是黑色，但 Java 实现的红黑树会使用 null 来代表空节点，因此我们在遍历 Java里的红黑树的时候会看不到叶子节点，而看到的是每个叶子节点都是红色的，这一点需要注意。
对于性质 5，这里我们需要注意的是，这里的描述是从任一节点，从任一节点到它的子树的每个叶子节点黑色节点的数量都是相同的，这个数量被称为这个节点的黑高。
如果我们从根节点出发到每个叶子节点的路径都包含相同数量的黑色节点，这个黑色节点的数量被称为树的黑色高度。树的黑色高度和节点的黑色高度是不一样的，这里要注意区分。
其实到这里有人可能会问了，红黑树的性质说了一大堆，那是不是说只要保证红黑树的节点是红黑交替就能保证树是平衡的呢？
其实不是这样的，我们可以看来看下面这张图：

左边的子树都是黑色节点，但是这个红黑树依然是平衡的，5 条性质它都满足。
这个树的黑色高度为 3，从根节点到叶子节点的最短路径长度是 2，该路径上全是黑色节点，包括叶子节点，从根节点到叶子节点最长路径为 4，每个黑色节点之间会插入红色节点。
通过上面的性质 4 和性质 5，其实上保证了没有任何一条路径会比其他路径长出两倍，所以这样的红黑树是平衡的。
其实这算是一个推论，红黑树在最差情况下，最长的路径都不会比最短的路径长出两倍。其实红黑树并不是真正的平衡二叉树，它只能保证大致是平衡的，因为红黑树的高度不会无限增高，在实际应用用，红黑树的统计性能要高于平衡二叉树，但极端性能略差。
想要彻底理解红黑树，除了上面说到的理解红黑树的性质以外，就是理解红黑树的插入操作了。
红黑树的插入和普通排序二叉树的插入基本一致，排序二叉树的要求是左子树上的所有节点都要比根节点小，右子树上的所有节点都要比跟节点大，当插入一个新的节点的时候，首先要找到当前要插入的节点适合放在排序二叉树哪个位置，然后插入当前节点即可。红黑树和排序二叉树不同的是，红黑树需要在插入节点调整树的结构来让树保持平衡。
一般情况下，红黑树中新插入的节点都是红色的，那么，为什么说新加入到红黑树中的节点要是红色的呢？
这个问题可以这样理解，我们从性质5中知道，当前红黑树中从根节点到每个叶子节点的黑色节点数量是一样的，此时假如新的黑色节点的话，必然破坏规则，但加入红色节点却不一定，除非其父节点就是红色节点，因此加入红色节点，破坏规则的可能性小一些。
接下来我们重点来讲红黑树插入新节点后是如何保持平衡的。
给定下面这样一颗红黑树：

当我们插入值为66的节点的时候，示意图如下：

很明显，这个时候结构依然遵循着上述5大特性，无需启动自动平衡机制调整节点平衡状态。
如果再向里面插入值为51的节点呢，这个时候红黑树变成了这样。

这样的结构实际上是不满足性质4的，红色两个子节点必须是黑色的，而这里49这个红色节点现在有个51的红色节点与其相连。
这个时候我们需要调整这个树的结构来保证红黑树的平衡。
首先尝试将49这个节点设置为黑色，如下示意图。

这个时候我们发现黑高是不对的，其中 60-56-45-49-51-null 这条路径有 4 个黑节点，其他路径的黑色节点是 3 个。
接着调整红黑树，我们再次尝试把45这个节点设置为红色的，如下图所示：

这个时候我们发现问题又来了，56-45-43 都是红色节点的，出现了红色节点相连的问题。
于是我们需要再把 56 和 43 设置为黑色的，如下图所示。

于是我们把 68 这个红色节点设置为黑色的。

对于这种红黑树插入节点的情况下，我们可以只需要通过变色就可以保持树的平衡了。但是并不是每次都是这么幸运的，当变色行不通的时候，我们需要考虑另一个手段就是旋转了。
例如下面这种情况，同样还是拿这颗红黑树举例。

现在这颗红黑树，我们现在插入节点65。

我们尝试把 66 这个节点设置为黑色，如下图所示。

这样操作之后黑高又出现不一致的情况了，60-68-64-null 有 3 个黑色节点，而60-68-64-66-null 这条路径有 4 个黑色节点，这样的结构是不平衡的。
或者我们把 68 设置为黑色，把 64 设置为红色，如下图所示：

但是，同样的问题，上面这颗红黑树的黑色高度还是不一致，60-68-64-null 和 60-68-64-66-null 这两条路径黑色高度还是不一致。
这种情况如果只通过变色的情况是不能保持红黑树的平衡的。
接下来我们讲讲红黑树的旋转，旋转分为左旋和右旋。
文字描述：逆时针旋转两个节点，让一个节点被其右子节点取代，而该节点成为右子节点的左子节点。
文字描述太抽象，接下来看下图片展示。
首先断开节点PL与右子节点G的关系，同时将其右子节点的引用指向节点C2；然后断开节点G与左子节点C2的关系，同时将G的左子节点的应用指向节点PL。

接下来再放下 gif 图，希望能帮助大家更好地理解左旋，图片来自网络。

文字描述：顺时针旋转两个节点，让一个节点被其左子节点取代，而该节点成为左子节点的右子节点。
右旋的图片展示：
首先断开节点G与左子节点PL的关系，同时将其左子节点的引用指向节点C2；然后断开节点PL与右子节点C2的关系，同时将PL的右子节点的应用指向节点G。

右旋的gif展示（图片来自网络）:

介绍完了左旋和右旋基本操作，我们来详细介绍下红黑树的几种旋转场景。
如下图所示的红黑树，我们插入节点是65。

操作步骤如下可以围绕祖父节点 69 右旋，再结合变色，步骤如下所示：

还是上面这颗红黑树，我们再插入节点 67。

这种情况我们可以这样操作，先围绕父节点 66 左旋，然后再围绕祖父节点 69 右旋，最后再将 67 设置为黑色，把 69 设置为红色，如下图所示。

如下图这种情况，我们要插入节点68。

这种情况，我们可以先围绕父节点 69 右旋，接着再围绕祖父节点 66 左旋，最后把 68 节点设置为黑色，把 66 设置为红色，我们的具体操作步骤如下所示。

还是来上面的图来举例，我们在这颗红黑树上插入节点 70 。

我们可以这样操作围绕祖父节点 66 左旋，再把旋转后的根节点 69 设置为黑色，把 66 这个节点设置为红色。具体可以参看下图：

看不懂代码的可以看下面的详解
TreeMap 的put方法。
TreeMap的插入节点和普通的排序二叉树没啥区别，唯一不同的是，在TreeMap 插入节点后会调用方法fixAfterInsertion(e)来重新调整红黑树的结构来让红黑树保持平衡。
我们重点关注下红黑树的fixAfterInsertion(e)方法，接下来我们来分别介绍两种场景来演示fixAfterInsertion(e)方法的执行流程。
同样是拿这颗红黑树举例，现在我们插入节点 51。

当我们需要插入节点51的时候，这个时候TreeMap 的 put 方法执行后会得到下面这张图。

接着调用fixAfterInsertion(e)方法，如下代码流程所示。

当第一次进入循环后，执行后会得到下面的红黑树结构

在把 x 重新赋值后，重新进入 while 循环，此时的 x 节点为 45 。

执行上述流程后，得到下面所示的红黑树结构。

这个时候x被重新赋值为60，因为60是根节点，所以会退出 while 循环。在退出循序后，会再次把根节点设置为黑色，得到最终的结构如下图所示。

最后经过两次执行while循环后，我们的红黑树会调整成现在这样的结构，这样的红黑树结构是平衡的，所以路径的黑高一致，并且没有红色节点相连的情况。
接下来我们再来演示第二种场景，需要结合变色和旋转一起来保持平衡。
给定下面这样一颗红黑树：

现在我们插入节点66，得到如下树结构。

同样地，我们进入fixAfterInsertion(e)方法。


最终我们得到的红黑树结构如下图所示：

调整成这样的结构我们的红黑树又再次保持平衡了。
演示 TreeMap 的流程就拿这两种场景举例了，其他的就不一一举例了。
因为之前的分享只整理了红黑树的插入部分，本来想着红黑树的删除就不整理了，有人跟我反馈说红黑树的删除相对更复杂，于是索性还是把红黑树的删除再整理下。
删除相对插入来说，的确是要复杂一点，但是复杂的地方是因为在删除节点的这个操作情况有很多种，但是插入不一样，插入节点的时候实际上这个节点的位置是确定的，在节点插入成功后只需要调整红黑树的平衡就可以了。
但是删除不一样的是，删除节点的时候我们不能简单地把这个节点设置为null，因为如果这个节点有子节点的情况下，不能简单地把当前删除的节点设置为null，这个被删除的节点的位置需要有新的节点来填补。这样一来，需要分多种情况来处理了。
直接删除根节点即可。
直接删除当前节点即可。
这个时候需要使用子节点来代替当前需要删除的节点，然后再把子节点删除即可。
给定下面这棵树，当我们需要删除节点69的时候。

首先用子节点代替当前待删除节点，然后再把子节点删除。

最终的红黑树结构如下面所示，这个结构的红黑树我们是不需要通过变色+旋转来保持红黑树的平衡了，因为将子节点删除后树已经是平衡的了。

还有一种场景是当我们待删除节点是黑色的，黑色的节点被删除后，树的黑高就会出现不一致的情况，这个时候就需要重新调整结构。
还是拿上面这颗删除节点后的红黑树举例，我们现在需要删除节点67。

因为67 这个节点的两个子节点都是null，所以直接删除,得到如下图所示结构：

这个时候我们树的黑高是不一致的，左边黑高是3，右边是2，所以我们需要把64节点设置为红色来保持平衡。

删除节点两个子节点都不为空的情况下，跟上面有一个节点不为空的情况下也是有点类似，同样是需要找能替代当前节点的节点，找到后，把能替代删除节点值复制过来，然后再把替代节点删除掉。
那么什么叫做前驱，什么叫做后继呢？
前驱是左子树中最大的节点，后继则是右子树中最小的节点。
前驱或者后继都是最接近当前节点的节点，当我们需要删除当前节点的时候，也就是找到能替代当前节点的节点，能够替代当前节点肯定是最接近当前节点。
在当前删除节点两个子节点不为空的场景下，我们需要再进行细分，主要分为以下三种情况。
如下面这样一棵树，我们需要删除节点64：

首先找到前驱节点，把前驱节点复制到当前节点：

接着删除前驱节点。

这个时候63和60这个节点都是红色的，我们尝试把60这个节点设置为红色即可使整个红黑树达到平衡。

前驱节点是黑色的，子节点都为空，这个时候操作步骤与上面基本类似。
如下操作步骤：

因为要删除节点64，接着找到前驱节点63，把63节点复制到当前位置，然后将前驱节点63删除掉，变色后出现黑高不一致的情况下，最后把63节点设置为黑色，把65节点设置为红色，这样就能保证红黑树的平衡。
给定下面这颗红黑树，我们需要删除节点64的时候。

同样地，我们找到64的前驱节点63，接着把63赋值到64这个位置。

然后删除前驱节点。

删除节点后不需要变色也不需要旋转即可保持树的平衡。
终于把红黑树的基本原理部分写完了，用了很多示意图，这篇文章是在之前分享的 ppt 上再整理出来，我觉得自己应该算是把基本操作讲明白了，整理这篇文章前前后后用了近一周左右，因为平时上班，基本上只有周末有时间才有时间整理，如有问题请留言讨论。
如果您觉得写得还可以，请您帮忙点个好看，您的点赞真的是对我最大的支持，也是我能继续写下去的动力，感谢。
基础数据结构",
别再问我什么是 trie 树了,"Trie 树大家了解一下原理和应用即可，有时候面试的时候会问到，下面我用面试的情景跟大家讲解 trie 树。

面试官：玩过王者荣耀吧？了解过敏感词过滤吗？，例如在游戏里，如果我们发送“你在干嘛？麻痹演员啊你？”，由于“麻痹”是一个敏感词，所以当你把聊天发出来之后，我们会用“**”来代表“麻痹”这次词，所以发送出来的聊天会变成这样：“你在干嘛？**演员啊你？”。
小秋：听说过啊，在各大社区也经常看到，例如评论一个问题等，一些粗话经常被过滤掉了。
面试官：嗯，如果我给你一段文字，以及给你一些需要过滤的敏感词，你会怎么来实现这个敏感词过滤的算法呢？例如我给你一段字符串“abcdefghi”,以及三个敏感词”de”, “bca”, “bcf”。
小秋：（敏感词过来算法？？不就是字符串匹配吗？）我可以通过字符串匹配算法，例如在字符串”abcdefghi”在查找是否存在字串“de”，如果找到了就把”de“用”**”代替。通过三次匹配之后，接变成这样了：“abc ** fghi”。
面试官：可以说说你采用哪种字符串匹配算法吗？
小秋：最简单的方法就是采用两个for循环保留求解了，不过每次匹配的都时间复杂度为O(n*m)，我可以采用 KMP 字符串匹配算法，这样时间复杂度是 O(m+n)。

  n 表示字符串的长度，m 表示每个敏感词的长度。

面试官：这是一个方法，对于敏感词过滤，你还有其他方法吗？
小秋：（其他方法？说实话，我也觉得不是采用这种 KMP 算法来匹配的了，可是，之前也没去了解过敏感词，这下要凉）对敏感词过来之前也没了解过，暂时没想到其他方法。
面试官：了解过 trie 树吗？
小秋：（嘿嘿，数据结构这方法，我得争气点）了解过，我还用代码实现过。
面试官：可以说说它的特点吗？
小秋：trie 树也称为字典树、单词查找树，最大的特点就是共享字符串的公共前缀来达到节省空间的目的了。例如，字符串 “abc”和”abd”构成的 trie 树如下：

trie 树的根节点不存任何数据，每整个个分支代表一个完整的字符串。像 abc 和 abd 有公共前缀 ab，所以我们可以共享节点 ab。如果再插入 abf，则变成这样：

如果我再插入 bc，则是这样（bc 和其他三个字符串没有公共前缀）

面试官：那如果再插入 “ab” 这个字符串呢？
小秋：差点说了，每个分支的内部可能也含有完整的字符串，所以我们可以对于那些是某个字符串结尾的节点做一个标记，例如 abc, abd,abf 都包含了字符串 ab,所以我们可以在节点 b 这里做一个标记。如下（我用红色作为标记）：

面试官：可以说说 trie 树有哪些应用吗？
小秋：trie 最大的特点就是利用了字符串的公共前缀，像我们有时候在百度、谷歌输入某个关键字的时候，它会给我们列举出很多相关的信息

这种就是通过 trie 树来实现的。
小秋：（嗯？ trie 又称为单词查找树，好像可以用 trie 来实现刚才的敏感词匹配？面试官无缘无故提 trie 树难道别有用意？）
面试官：刚才的敏感词过滤，其实也可以采用 trie 来实现，你知道怎么实现吗？
小秋：（果然，面试官真是个好人啊，直接提示了，要是还不知道怎么实现，那不真凉？）我想想……..我知道了，
我可以这样来实现：
先把你给我的三个敏感词：”de”, “bca”, “bcf” 建立一颗 trie 树，如下：

接着我们可以采用三个指针来遍历，我直接用上面你给你例子来演示吧。
1、首先指针 p1 指向 root，指针 p2 和 p3 指向字符串第一个字符

2、然后从字符串的 a 开始，检测有没有以 a 作为前缀的敏感词，直接判断 p1 的孩子节点中是否有 a 这个节点就可以了，显然这里没有。接着把指针 p2 和 p3 向右移动一格。

3、然后从字符串 b 开始查找，看看是否有以 b 作为前缀的字符串，p1 的孩子节点中有 b，这时，我们把 p1 指向节点 b，p2 向右移动一格，不过，p3不动。

4、判断 p1 的孩子节点中是否存在 p2 指向的字符c，显然有。我们把 p1 指向节点 c，p2 向右移动一格，p3不动。

5、判断 p1 的孩子节点中是否存在 p2 指向的字符d，这里没有。这意味着，不存在以字符b作为前缀的敏感词。这时我们把p2和p3都移向字符c，p1 还是还原到最开始指向 root。

6、和前面的步骤一样，判断有没以 c 作为前缀的字符串，显然这里没有，所以把 p2 和 p3 移到字符 d。

7、然后从字符串 d 开始查找，看看是否有以 d 作为前缀的字符串，p1 的孩子节点中有 d，这时，我们把 p1 指向节点 d，p2 向右移动一格，不过，p3和刚才一样不动。（看到这里，我猜你已经懂了）

8、判断 p1 的孩子节点中是否存在 p2 指向的字符e，显然有。我们把 p1 指向节点 e，并且，这里e是最后一个节点了，查找结束，所以存在敏感词de，即 p3 和 p2 这个区间指向的就是敏感词了，把 p2 和 p3 指向的区间那些字符替换成 *。并且把 p2 和 p3 移向字符 f。如下：

9、接着还是重复同样的步骤，知道 p3 指向最后一个字符。
面试官：可以说说时间复杂度吗？
小秋：如果敏感词的长度为 m，则每个敏感词的查找时间复杂度是 O(m)，字符串的长度为 n，我们需要遍历 n 遍，所以敏感词查找这个过程的时间复杂度是 O(n * m)。如果有 t 个敏感词的话，构建 trie 树的时间复杂度是 O(t * m)。

  这里我说明一下，在实际的应用中，构建 trie 树的时间复杂度我觉得可以忽略，因为 trie 树我们可以在一开始就构建了，以后可以无数次重复利用的了。而刚才的 kmp 算法时间复杂度是 t *(m+n)，不过kmp需要维护 next 数组比较费空间，而且在实际情况中,敏感词的数量 t 是比较大，而 n 反而比较小的吧。

10、如果让你来 构建 trie 树，你会用什么数据结构来实现？
小秋：我一般使用 Java，我会采用 HashMap 来实现，因为一个节点的字节点个数未知，采用 HashMap 可以动态拓展，而且可以在 O(1) 复杂度内判断某个子节点是否存在。
面试官：嗯，回去等通知吧。
今天主要将了 trie 树以及 trie 树的一些应用，还要就是如何通过 trie 树来实现敏感词的过滤，至于代码的实现，我这里就不给出了，在实现的时候，为了防止这种”麻 痹”或者“麻￥痹”等，我们也要对特殊字符进行过滤等，有兴趣的可以去实现一波。
基础数据结构",
Hash设计原理（上）,"理解HashMap底层，首先应该理解Hash函数，今天我们聊聊什么是Hash函数以及Hash函数的设计、
查找速度的困扰
算法国自建立起，就以快速为至高的荣誉，O(n^2) 时间复杂度的设计常常被人嫌弃，一般都想着弄个O(logn)
算法国最近遇到了一个问题，就是随着处理数据的逐步增大，查找的时间越来越大了
之前用的数组和链表，最后改成二叉查找树，可是这些都需要和其中的元素进行比较，比较的次数越多，查询的速度就越慢
国王想着能不能有一种办法，查找某个元素的时候，不需要比较，直接就得到这个元素，时间复杂度直接就为O(1),和集合中的元素个数就没有关系了
国王召集各大臣讨论此事
“如果不需要比较，我们可以借鉴数组下标访问的思路来做”，经验老道的王大臣发话了
“哦，怎么个设计？”何大臣问道
“你看，我们要访问数组中某个元素怎么访问？”
“只需要知道起始位置和下标值就可以了，不管数组中有多少个元素，都可以一次访问到，这正是因为起始位置和下标值组成了元素的存储位置，从这个存储位置就可以直接找到元素”，王大臣自问自答起来
左丞相接着道，“所以说，给定一个元素，我们只要得到该元素的位置，也就是说将元素和元素位置建立一种一一对应的关系，就可以迅速定位要查的元素了”，说着说着，王大臣在纸上画起了图

“现在问题的关键就是输入55，我们如何知道元素55的存储位置”，何大臣说道
“对”，王大臣答到
“如果输入的数为正整数的话，我们完全可以用数组存储，用这个输入的正整数作为数组的下标来存取元素”，王大臣说道
“比如说55，我们就存到下标为55的位置，这样是不是很简单”，王大臣举了一个例子，顺便画了一个图

Hash函数的出现
“这也太理想了吧，现实生活中要存储的元素（key）的取值范围一般很大，比如说正整数，不可能为无穷多个正整数分配无穷大的数组吧，这不现实
就算有无穷大的数组，但实际存储的元素的范围可能很小，比如输入大多数在1-100之间，只有极少数的是其他数，那预先分配的那些空间不是浪费了吗？”，何大臣立马驳回了王大臣的想法
“那依你看，如何是好？”王大臣问道
“我看，既然输入的元素的范围可能很大甚至无穷，而我们的内存有限，所以说我们需要一种函数映射关系，将这些无限的元素映射到我们有限的内存地址上”，说着说着画了一个图

“很显然，多个数据有可能被映射到同一个地址上，我们暂且称为冲突吧”，何大臣说道
“我们给这个函数映射起一个名字，就叫Hash函数吧，这个Hash函数代表着一类函数，即把任意范围的元素可以通过映射关系压缩成固定范围的元素”何大臣说道，大家一致同意
Hash函数的选择
“那现在的问题就很清晰了，一个是这个Hash函数该选择什么样具体的函数，另一个就是出现冲突该怎么办？”王大臣说道
“如果是正整数，我们可以用这个正整数数除以某个数，取其余数，即我们常用的 k % m，k为正整数，m 为除数
这样一来，范围就缩小了很多，比如说 15%10=5,26%10=6,…,所有的正整数经过运算，都变成了 0-9 范围之间的数了，这样范围就缩小了很多”何大臣发挥了他数学的才能
大家一致称赞
m 的选择
“如果是这种做法，m 的选择就非常重要了，如果 k 值分布均匀还无所谓，如果 k 值具有某些特征
比如说 k 的个位基本上不变，而高位分布均匀，如 15,25,45,65,85,95,155,这么一组数据，经过你刚才的 k%10不就全部落在5这个位置上了吗？”一直没有说话的另一个李大臣发话了
这个李大臣虽少言寡语，但非常有才能
“对对对”，何大臣连声说道
“我们必须要使得经过Hash函数后关键字的分布均匀，尽量减少冲突，所以针对不同类型的关键字要有自己特定的Hash函数，整数应该有整数的Hash函数，字符串应该有字符串的Hash函数
就算针对同一类型的关键字，如果它具有某种规律，我们也要具体情况具体设计，刚才李大臣说的那种情况，我们就得选取其他分布均匀的高位来进行Hash”右丞相补充道
“那为何不优化一下我们的Hash函数来使得关键字分布均匀，比如说 m 取一个素数，这样就不会出现刚才的情况了，比如说m取11，15,25,45,65,85,95,155，这些数Hash后就会变成这个”，李大臣说着说着画了一个表

“这样就分布均匀，冲突很少，所以我们只需要用一个长度为11的数组来存储就行，只比你原先的10多了一个长度”李大臣补充道
“如果事先知道元素的分布情况，那我们可以针对特定的元素来设计Hash函数，如果不知道的话，那我们就提前想一个比较好的Hash函数，来应对各种可能出现的情况
不管怎样，我们最终的目的就是让各个关键字均匀的Hash到不同的位置”，国王看天色渐晚，总结了一下，然后说道，今天就到这里，明天再议
基础数据结构",
Hash设计原理（中）,"理解HashMap底层，首先应该理解Hash函数，今天我们聊聊Hash函数出现冲突的解决办法（此故事为连载形式，若没看上篇，可点击此处神速Hash阅读）
神速Hash阅读
次日清晨，大臣们按时上朝，接着讨论昨日的话题
“昨日Hash函数的选择我们已经有了具体的方案了，那就只剩下冲突的解决问题了”，王大臣率先发话
“要解决冲突其实也不难，既然会有多个元素被Hash到同一个位置，而这个位置只能存储一个元素，那么我让这个位置可以存储多个元素不就可以了吗？”，何大臣说道
“哦，怎么个弄法？”王大臣问道
“用链表啊，来一个元素加一个，让这个位置存储一个指针，指向一个链表，让所有相同位置的元素都放在这个链表中”，何大臣回答道，接着又画了一个图

“在存储的时候，如果多个元素被Hash到同一位置，那么就加入到该位置所指向的链表中，如果该位置没有元素，则为null(指向空)”，何大臣解释道
“那个 1 和 6 谁先存放进来呢？”，思维缜密的王大人问道
“这个当然是6了，因为这个插入链表的时候要采用‘头插’的方式，也就是插入链表的最前面（图中里数组最近的元素）”，何大人说道
“哦，这是为什么呢？”，王大人追问道
“因为经常发生这样的事情：新加入的元素很可能被再次访问到，所以放到头的话，如果查找就不用再遍历链表了”，见多识广的何大人解释道
“这样解决冲突固然好，但是也有瓶颈啊”，寡言的李大臣又发言了
“哦，什么瓶颈？”，何大人问道
“你看啊，假设咱们的Hash函数设计的非常好，能够将元素均匀Hash（散列）开来，但是当我们实际存入的值越来越多的时候，这个链表也势必越来越长，那当我们进行查找的时候，势必就会遍历链表，效率也就越来越慢”，李大臣回应道，顺便画了一个图

“这样的话，随着链表的不断增长，查询某一个元素的时间也就增加了，如果链表长度远远大于数组长度，不就和用链表存储一样了吗？”，李大臣说道
“恩恩，对，李大臣说的极是，李大臣有何高见？”，何大臣问道
“现在只能扩大数组的长度大约为原来的两倍
然后选取一个相关的新的Hash函数（比如之前使用 key % m，现在只改变一下m的值）
将旧Hash表中所有的元素通过新的Hash函数计算出新的Hash值，并将其插入到新表中（仍然使用链表），这就叫rehash吧”，李大臣说完又画了一个图

“这里的数组就扩大了近两倍，由于要大小要选素数，那就选原数组大小两倍后的第一个素数7，旧Hash表和新Hash表采用了不同的Hash函数，但相关，只是m的取值变了”李大臣解释道
“哦，这样做确实是一种办法，但是问题随之而来，就是什么时候开始rehash”，何大臣说道
“我们可以定义这样一个变量 α = 所有元素个数/数组的大小，我们叫它装载因子吧，它代表着我们的Hash表（也就是数组）的装满程度，在这里也代表链表的平均长度
比如说，我们的数组大小为 5 ，我们给里面存入 3个元素，那么 α = 3/5 =0.6， 这个Hash表装满程度为60%，平均每条链有0.6个元素，当然 α 也可以等于和大于 1 ”,李大臣说道
“哦，引入这个装载因子有何用意？”，何大臣问道
“这个装载因子代表了Hash表的装满程度，这里也可以代表链表的平均长度，那么也就可以代表查询时的时间长短了“
基于此，我们为了不让查询时间长，也就是查询性能低，我们可以设置一个临界 α 值，当随着存入元素导致 α 大于这个临界 α 值的时候
我们可以通过rehash来调整当前的 α 值，使之低于我们设定的 临界 α 值，从而使我们的查询性能保持在较好的范围之内”，李大臣答道
“比如说，我们设定 临界 α = 0.7，对于一个Hash表大小为5的Hash表而言
当存入存入第四个元素的时候，α 就超出了临界 α 值，我们可以将数组长度变为11进行rehash（因为11是原表两倍后的第一个素数），使得装载因子 α 小于 0.7”，李大臣举了一个例子并画了一个图

“通过rehash我们可以使得装载因子在一定范围内，那我们的查询性能也就得到了保证了”，李大臣说道
“哦，那这个 临界的 α 值应该选择多大呢？”何大臣追问道
“这个 临界 α 如果选的小了，那数组的空间利用率就会太低，就比如说数组大小为100，α = 0.01，那装满程度为1%，99%还没有被利用
如果 α 太大了，那冲突就会很多，比如说 数组大小为 5，α = 10, 那平均每条链有10个元素，装满程度为1000%
即使Hash函数设计的合理，基本上每次存放元素的时候就会冲突，所以鉴于两者之间我觉得 0.6 – 0.9 之间是一个不错的选择，不妨选0.75吧”，李大臣回答道
大家一致同意
“这两天辛苦众爱卿了，你们的方案非常好，我也听了听
我们这种用Hash函数将关键字和关键字地址建立起来的映射，对我们的查找非常有帮助
其中非常重要的关键点就是：哈希函数的选择、处理冲突的方法以及装载因子调整，接下来我们把这个点子应用到我国的查找行业，我相信一定会有很大的提高”，国王做了最后的总结
基础数据结构",
Hash设计原理之开放地址法（下）,"比如说我的输入是任意一个自然数（0,1,2,3…），而我要求经过一个函数后我的输出的数的范围要在0-9这样一个范围之间。

很容易想到，我们可以使用Hash函数：

其中key就是输入
在哈希表（散列表）里，Hash函数的作用就是将关键字Key转化为一个固定长度数组的下标，以便存取键值对<Key,Value>




链地址法可看：神速Hash
神速Hash


所谓开放地址法就是发生冲突时在散列表（也就是数组里）里去寻找合适的位置存取对应的元素。



最容易想到的就是当前位置冲突了，那我就去找相邻的下一个位置。
就拿放入元素举例吧，当你放入<a,101>到下标为2的位置后，另一个<c,103>键值对也落入了这个位置，那么它就向后依次加一寻找合适的位置，然后把<c,103>放入进去。


我们把这种方法称作线性探测法,我们可以将Hash以及寻找位置的过程抽象成一个函数：

所以关键字要进行查找或者插入，首先看(hash1(key)+0)%7 位置是自己最终的位置吗？如果有冲突，就探测（查看）下一个位置:(hash1(key)+1)%7。依次进行

  所谓探测，就是在插入的时候检查哪个位置可以插入，或者查找时查找哪个位置是要查找的键值对，本质就是探寻这个键值对最终的位置。

但是这样会有一个问题，就是随着键值对的增多，会在哈希表里形成连续的键值对

这样的话，当插入元素时，任意一个落入这个区间的元素都要一直探测到区间末尾，并且最终将自己加入到这个区间内。这样就会导致落在区间内的关键字Key要进行多次探测才能找到合适的位置，并且还会继续增大这个连续区间，使探测时间变得更长，这样的现象被称为“一次聚集（primary clustering）”




其实我们可以让它按照 i^2 的规律来跳跃探测


这样的话，元素就不会聚集在某一块区域了，我们把这种方法称为平方探测法
同样我们可以抽象成下面的函数：

其实可以扩展到更一般的形式：

虽然平方探测法解决了线性探测法的一次聚集，但是它也有一个小问题，就是关键字key散列到同一位置后探测时的路径是一样的。

这样对于许多落在同一位置的关键字而言，越是后面插入的元素，探测的时间就越长。
这种现象被称作“二次聚集(secondary clustering)”,其实这个在线性探测法里也有。
这种现象出现的原因是由于对于落在同一个位置的关键字我们采取了一个依赖 i 的函数（i或者i^2）来进行探测，它不会因为关键字的不同或其他因素而改变探测的路径。那么我们是不是可以让探测的方法依赖于关键字呢？
答案是可以的，我们可以再弄另外一个Hash函数，对落在同一个位置的关键字进行再次的Hash,探测的时候就用依赖这个Hash值去探测，比如我们可以使用下面的函数：

经过hash1的散列后，会定位到某一个地址，如果这个地址冲突，那么就按照1hash2(key)、2hash2(key)… 的偏移去探测合适的位置。

由于Hash2函数不同于Hash1,所以两个不同的关键字Hash1值和Hash2值同时相同的概率就会变得非常低。
这样就避免了二次聚集，但同时也付出了计算另一个散列函数Hash2的代价。




基础数据结构",
